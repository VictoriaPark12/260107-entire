"""
íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ì„œë¹„ìŠ¤
íŒë‹¤ìŠ¤, ë„˜íŒŒì´, ì‚¬ì´í‚·ëŸ°ì„ ì‚¬ìš©í•œ ë°ì´í„° ì²˜ë¦¬ ë° ë¨¸ì‹ ëŸ¬ë‹ ì„œë¹„ìŠ¤
"""
import sys
from pathlib import Path
from typing import List, Dict, Optional, Any, ParamSpecArgs
import pandas as pd
import numpy as np
import pickle
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from app.titanic.titanic_dataset import TitanicDataSet

# ê³µí†µ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

try:
    from common.utils import setup_logging
    logger = setup_logging("titanic_service")
except ImportError:
    import logging
    logger = logging.getLogger("titanic_service")

from app.titanic.titanic_method import TitanicMethod


class TitanicService:
    """íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ì²˜ë¦¬ ë° ë¨¸ì‹ ëŸ¬ë‹ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.processed_data = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.y_train_full = None  # Survived ë¼ë²¨ ì €ì¥
        self.models = {}
        self.evaluation_results = {}


    def preprocess(self) -> Dict[str, Any]:
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° ì •ë³´ ë°˜í™˜"""
        def clean_for_json(obj):
            """DataFrameì˜ NaN, inf ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜í•˜ê³  booleanì„ intë¡œ ë³€í™˜í•˜ì—¬ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ í•¨"""
            if isinstance(obj, bool):
                return 1 if obj else 0
            elif isinstance(obj, (np.integer, np.floating)):
                if np.isnan(obj) or np.isinf(obj):
                    return None
                return float(obj) if isinstance(obj, np.floating) else int(obj)
            elif isinstance(obj, dict):
                return {k: clean_for_json(v) for k, v in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [clean_for_json(item) for item in obj]
            elif isinstance(obj, pd.Series):
                return clean_for_json(obj.to_dict())
            elif isinstance(obj, pd.DataFrame):
                return clean_for_json(obj.to_dict('records'))
            return obj
        
        try:
            logger.info("\n" + "="*80)
            logger.info("ì „ì²˜ë¦¬ ì‹œì‘")
            logger.info("="*80)
            the_method = TitanicMethod()
            
            # CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
            base_path = Path(__file__).parent
            train_csv_path = base_path / 'train.csv'
            test_csv_path = base_path / 'test.csv'
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not train_csv_path.exists():
                raise FileNotFoundError(f"Train CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_csv_path}")
            if not test_csv_path.exists():
                raise FileNotFoundError(f"Test CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_csv_path}")
            
            # Train ë°ì´í„° ë¡œë“œ
            df_train = the_method.read_csv(str(train_csv_path))
            
            # Test ë°ì´í„° ë¡œë“œ
            df_test = the_method.read_csv(str(test_csv_path))
            
            # ì›ë³¸ ë°ì´í„° ìš”ì•½ ì •ë³´ ì¶œë ¥
            if 'Survived' in df_train.columns:
                total_passengers = len(df_train)
                survived_count = int(df_train['Survived'].sum())
                death_count = total_passengers - survived_count
                survived_rate = (survived_count / total_passengers * 100) if total_passengers > 0 else 0
                death_rate = (death_count / total_passengers * 100) if total_passengers > 0 else 0
                
                logger.info("\n" + "="*80)
                logger.info("íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ì „ì²´ ìš”ì•½")
                logger.info("="*80)
                logger.info(f"ì „ì²´ ìŠ¹ê° ìˆ˜: {total_passengers}ëª…")
                logger.info(f"ìƒì¡´ì: {survived_count}ëª… ({survived_rate:.2f}%)")
                logger.info(f"ì‚¬ë§ì: {death_count}ëª… ({death_rate:.2f}%)")
                logger.info(f"ì»¬ëŸ¼ ìˆ˜: {len(df_train.columns)}ê°œ")
                logger.info(f"ì»¬ëŸ¼ ëª©ë¡: {', '.join(df_train.columns.tolist())}")
            
            # DataFrame ìƒì„±
            this_train = the_method.create_df(df_train, 'Survived')
            if 'Survived' in df_test.columns:
                this_test = the_method.create_df(df_test, 'Survived')
            else:
                this_test = df_test.copy()
            
            # this ê°ì²´ ì´ˆê¸°í™”
            this = TitanicDataSet()
            this.train = this_train
            this.test = this_test
            
            # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ê³ ì • ë¡œì§)
            drop_features = ['SibSp', 'Parch', 'Cabin', 'Ticket']
            this = the_method.drop_features(this, *drop_features)
            this = the_method.pclass_ordinal(this)
            this = the_method.fare_ratio(this)
            this = the_method.embarked_nominal(this)
            this = the_method.gender_nominal(this)
            this = the_method.age_ratio(this)
            this = the_method.title_nominal(this)
            drop_name = ['Name']
            this = the_method.drop_features(this, *drop_name)
            
            # booleanê³¼ ë¬¸ìì—´ ì»¬ëŸ¼ì„ intë¡œ ë³€í™˜í•˜ê³  ì›ë³¸ ë¬¸ìì—´ ì»¬ëŸ¼ ì œê±°
            def convert_to_int(df: pd.DataFrame) -> pd.DataFrame:
                """booleanê³¼ ë¬¸ìì—´ ì»¬ëŸ¼ì„ intë¡œ ë³€í™˜í•˜ê³  ì›ë³¸ ë¬¸ìì—´ ì»¬ëŸ¼ ì œê±°"""
                df = df.copy()
                
                # ì œê±°í•  ì›ë³¸ ë¬¸ìì—´ ì»¬ëŸ¼ ëª©ë¡
                cols_to_drop = []
                converted_cols = []
                
                # boolean ì»¬ëŸ¼ë“¤ì„ intë¡œ ë³€í™˜
                for col in df.columns:
                    if df[col].dtype == bool or df[col].dtype == 'bool':
                        df[col] = df[col].astype(int)
                        converted_cols.append(f"{col} (bool->int)")
                    elif df[col].dtype == object or pd.api.types.is_categorical_dtype(df[col]):
                        # ë¬¸ìì—´ ì»¬ëŸ¼ ì²˜ë¦¬ (object íƒ€ì… ë˜ëŠ” categorical íƒ€ì…)
                        if col == 'gender' or col == 'Sex':
                            cols_to_drop.append(col)
                        elif col == 'Age_band':
                            cols_to_drop.append(col)
                        elif col == 'Embarked':
                            cols_to_drop.append(col)
                        elif col == 'Title':
                            cols_to_drop.append(col)
                        elif col not in ['PassengerId', 'Pclass', 'Age', 'Fare']:  # ê¸°ë³¸ ìˆ«ì ì»¬ëŸ¼ ì œì™¸
                            # ê¸°íƒ€ ë¬¸ìì—´ ì»¬ëŸ¼ë„ ì œê±°
                            cols_to_drop.append(col)
                
                # ì›ë³¸ ë¬¸ìì—´ ì»¬ëŸ¼ ì œê±°
                if cols_to_drop:
                    df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])
                
                return df
            
            # Trainê³¼ Test ë°ì´í„°ë¥¼ intë¡œ ë³€í™˜
            this.train = convert_to_int(this.train)
            this.test = convert_to_int(this.test)
            
            # ìµœì¢… null ê°œìˆ˜ ê³„ì‚°
            train_null_count = int(this.train.isnull().sum().sum())
            test_null_count = int(this.test.isnull().sum().sum())
            
            # Train ì •ë³´ ì¶œë ¥
            logger.info(" ğŸ˜ğŸ˜ğŸ˜íŠ¸ë ˆì¸ ì „ì²˜ë¦¬ ì™„ë£Œ")
            logger.info(f"\n1. Train ì˜ type \n {type(this.train)}")
            logger.info(f"\n2. Train ì˜ column \n {this.train.columns}")
            logger.info(f"\n3. Train ì˜ ìƒìœ„ 5ê°œ í–‰\n {this.train.head(5)}")
            logger.info(f"\n4. Train ì˜ null ì˜ ê°¯ìˆ˜\n {train_null_count}ê°œ")
            
            # Test ì •ë³´ ì¶œë ¥
            logger.info("ğŸ¤¢ğŸ¤¢ğŸ¤¢ í…ŒìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì™„ë£Œ")
            logger.info(f"\n1. Test ì˜ type \n {type(this.test)}")
            logger.info(f"\n2. Test ì˜ column \n {this.test.columns}")
            logger.info(f"\n3. Test ì˜ ìƒìœ„ 5ê°œ í–‰\n {this.test.head(5)}")
            logger.info(f"\n4. Test ì˜ null ì˜ ê°¯ìˆ˜\n {test_null_count}ê°œ")
            
            # ê²°ê³¼ ë°˜í™˜
            result = {
                "train": {
                    "type": str(type(this.train)),
                    "columns": this.train.columns.tolist(),
                    "head": clean_for_json(this.train.head(5).to_dict('records')),
                    "null_count": train_null_count
                },
                "test": {
                    "type": str(type(this.test)),
                    "columns": this.test.columns.tolist(),
                    "head": clean_for_json(this.test.head(5).to_dict('records')),
                    "null_count": test_null_count
                }
            }
            
            logger.info("\n" + "="*80)
            logger.info("ì „ì²˜ë¦¬ ì™„ë£Œ")
            logger.info("="*80 + "\n")
            
            # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ (ëª¨ë¸ë§/í•™ìŠµ/í‰ê°€ìš©)
            self.processed_data = this
            if 'Survived' in df_train.columns:
                self.y_train_full = df_train['Survived']
            
            return result
            
        except FileNotFoundError as e:
            logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {e}")
            raise
        except Exception as e:
            logger.error(f"ì „ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def modeling(self):
        logger.info("ğŸ˜ğŸ˜ ëª¨ë¸ë§ ì‹œì‘")
        
        if self.processed_data is None:
            raise ValueError("ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. preprocess()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        try:
            # ëª¨ë¸ ìƒì„± (ëª¨ë“  ëª¨ë¸)
            self.models = {
                'DecisionTree': DecisionTreeClassifier(random_state=42),
                'RandomForest': RandomForestClassifier(n_estimators=13, random_state=42),
                'NaiveBayes': GaussianNB(),
                'KNN': KNeighborsClassifier(n_neighbors=13),
                'SVM': SVC(random_state=42)
            }
            logger.info(f"ëª¨ë¸ ìƒì„± ì™„ë£Œ: {list(self.models.keys())}")
            logger.info("ğŸ˜ğŸ˜ ëª¨ë¸ë§ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def learning(self):
        logger.info("ğŸ˜ğŸ˜ í•™ìŠµ ì‹œì‘")
        
        if self.processed_data is None:
            raise ValueError("ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. preprocess()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        if not self.models:
            raise ValueError("ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. modeling()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì¤€ë¹„
        X = self.processed_data.train.copy()
        
        # Survived ë¼ë²¨ í™•ì¸
        if self.y_train_full is None:
            raise ValueError("Survived ë¼ë²¨ì´ ì—†ìŠµë‹ˆë‹¤. preprocess()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        y = self.y_train_full.copy()
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ í™•ì¸ ë° ì œê±° (ëª¨ë¸ í•™ìŠµì„ ìœ„í•´)
        object_cols = [col for col in X.columns if X[col].dtype == object]
        if object_cols:
            logger.warning(f"ë¬¸ìì—´ ì»¬ëŸ¼ ë°œê²¬ ë° ì œê±°: {object_cols}")
            X = X.drop(columns=object_cols)
        
        # ë°ì´í„° í¬ê¸° í™•ì¸
        logger.info(f"ì „ì²´ ë°ì´í„° í¬ê¸°: X={X.shape}, y={y.shape}")
        logger.info(f"Survived ë¼ë²¨ ë¶„í¬: ìƒì¡´={int(y.sum())}ëª…, ì‚¬ë§={int((y == 0).sum())}ëª…")
        logger.info(f"í”¼ì²˜ ì»¬ëŸ¼: {list(X.columns)}")
        
        # Train/Validation ë¶„í• 
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        logger.info(f"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {self.X_train.shape}, ë¼ë²¨ í¬ê¸°: {self.y_train.shape}")
        logger.info(f"ê²€ì¦ ë°ì´í„° í¬ê¸°: {self.X_test.shape}, ë¼ë²¨ í¬ê¸°: {self.y_test.shape}")
        
        # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        models_dir = Path(__file__).parent / 'models'
        models_dir.mkdir(exist_ok=True)
        
        # ê° ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
        for name, model in self.models.items():
            logger.info(f"{name} ëª¨ë¸ í•™ìŠµ ì¤‘...")
            try:
                model.fit(self.X_train, self.y_train)
                logger.info(f"{name} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
                
                # ëª¨ë¸ ì €ì¥
                model_path = models_dir / f'{name}_model.pkl'
                with open(model_path, 'wb') as f:
                    pickle.dump(model, f)
                logger.info(f"{name} ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
            except Exception as e:
                logger.error(f"{name} ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
                raise
        
        logger.info("ğŸ˜ğŸ˜ í•™ìŠµ ì™„ë£Œ")

    def evaluate(self):
        logger.info("ğŸ˜ğŸ˜ í‰ê°€ ì‹œì‘")
        
        if self.processed_data is None:
            raise ValueError("ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. preprocess()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        if self.y_train_full is None:
            raise ValueError("Survived ë¼ë²¨ì´ ì—†ìŠµë‹ˆë‹¤. preprocess()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì¤€ë¹„
        X = self.processed_data.train.copy()
        y = self.y_train_full.copy()
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ í™•ì¸ ë° ì œê±° (ëª¨ë¸ í•™ìŠµì„ ìœ„í•´)
        object_cols = [col for col in X.columns if X[col].dtype == object or pd.api.types.is_categorical_dtype(X[col])]
        if object_cols:
            logger.warning(f"ë¬¸ìì—´ ì»¬ëŸ¼ ë°œê²¬ ë° ì œê±°: {object_cols}")
            X = X.drop(columns=object_cols)
        
        the_method = TitanicMethod()
        self.evaluation_results = {}
        
        # K-Fold êµì°¨ ê²€ì¦ìœ¼ë¡œ ê° ëª¨ë¸ í‰ê°€
        try:
            # ê²°ì •íŠ¸ë¦¬
            accuracy_dtree = the_method.accuracy_by_dtree(X, y)
            self.evaluation_results['DecisionTree'] = accuracy_dtree / 100
            logger.info(f'ê²°ì •íŠ¸ë¦¬ í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {accuracy_dtree}%')
            print(f'ê²°ì •íŠ¸ë¦¬ í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {accuracy_dtree}%')
        except Exception as e:
            logger.error(f"ê²°ì •íŠ¸ë¦¬ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
        
        try:
            # ëœë¤í¬ë ˆìŠ¤íŠ¸
            accuracy_rforest = the_method.accuracy_by_rforest(X, y)
            self.evaluation_results['RandomForest'] = accuracy_rforest / 100
            logger.info(f'ëœë¤í¬ë ˆìŠ¤íŠ¸ í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {accuracy_rforest}%')
            print(f'ëœë¤í¬ë ˆìŠ¤íŠ¸ í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {accuracy_rforest}%')
        except Exception as e:
            logger.error(f"ëœë¤í¬ë ˆìŠ¤íŠ¸ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
        
        try:
            # ë‚˜ì´ë¸Œë² ì´ì¦ˆ
            accuracy_nb = the_method.accuracy_by_nb(X, y)
            self.evaluation_results['NaiveBayes'] = accuracy_nb / 100
            logger.info(f'ë‚˜ì´ë¸Œë² ì´ì¦ˆ í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {accuracy_nb}%')
            print(f'ë‚˜ì´ë¸Œë² ì´ì¦ˆ í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {accuracy_nb}%')
        except Exception as e:
            logger.error(f"ë‚˜ì´ë¸Œë² ì´ì¦ˆ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
        
        try:
            # KNN
            accuracy_knn = the_method.accuracy_by_knn(X, y)
            self.evaluation_results['KNN'] = accuracy_knn / 100
            logger.info(f'KNN í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {accuracy_knn}%')
            print(f'KNN í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {accuracy_knn}%')
        except Exception as e:
            logger.error(f"KNN í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
        
        try:
            # SVM
            accuracy_svm = the_method.accuracy_by_svm(X, y)
            self.evaluation_results['SVM'] = accuracy_svm / 100
            logger.info(f'SVM í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {accuracy_svm}%')
            print(f'SVM í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {accuracy_svm}%')
        except Exception as e:
            logger.error(f"SVM í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
        
        logger.info("ğŸ˜ğŸ˜ í‰ê°€ ì™„ë£Œ")
        return self.evaluation_results


    def submit(self):
        """Kaggle ì œì¶œìš© ëª¨ë¸ ìƒì„± ë° ì €ì¥"""
        logger.info("ğŸ˜ğŸ˜ ì œì¶œ ì‹œì‘")
        
        if self.processed_data is None:
            raise ValueError("ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. preprocess()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        if self.y_train_full is None:
            raise ValueError("Survived ë¼ë²¨ì´ ì—†ìŠµë‹ˆë‹¤. preprocess()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ì „ì²´ train ë°ì´í„° ì¤€ë¹„
        X_train_full = self.processed_data.train.copy()
        y_train_full = self.y_train_full.copy()
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ í™•ì¸ ë° ì œê±°
        object_cols = [col for col in X_train_full.columns if X_train_full[col].dtype == object or pd.api.types.is_categorical_dtype(X_train_full[col])]
        if object_cols:
            logger.warning(f"ë¬¸ìì—´ ì»¬ëŸ¼ ë°œê²¬ ë° ì œê±°: {object_cols}")
            X_train_full = X_train_full.drop(columns=object_cols)
        
        # test ë°ì´í„° ì¤€ë¹„
        X_test = self.processed_data.test.copy()
        object_cols_test = [col for col in X_test.columns if X_test[col].dtype == object or pd.api.types.is_categorical_dtype(X_test[col])]
        if object_cols_test:
            X_test = X_test.drop(columns=object_cols_test)
        
        # trainê³¼ testì˜ ì»¬ëŸ¼ì„ ë™ì¼í•˜ê²Œ ë§ì¶”ê¸°
        common_cols = [col for col in X_train_full.columns if col in X_test.columns]
        X_train_full = X_train_full[common_cols]
        X_test = X_test[common_cols]
        
        logger.info(f"ì „ì²´ í•™ìŠµ ë°ì´í„° í¬ê¸°: X={X_train_full.shape}, y={y_train_full.shape}")
        logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: X={X_test.shape}")
        logger.info(f"í”¼ì²˜ ì»¬ëŸ¼: {list(X_train_full.columns)}")
        
        # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        models_dir = Path(__file__).parent / 'models'
        models_dir.mkdir(exist_ok=True)
        
        # ëª¨ë“  ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
        kaggle_models = {
            'DecisionTree': DecisionTreeClassifier(random_state=42),
            'RandomForest': RandomForestClassifier(n_estimators=13, random_state=42),
            'NaiveBayes': GaussianNB(),
            'KNN': KNeighborsClassifier(n_neighbors=13),
            'SVM': SVC(random_state=42)
        }
        
        results = {}
        
        # ê° ëª¨ë¸ë³„ë¡œ í•™ìŠµ, ì €ì¥, ì˜ˆì¸¡ ìˆ˜í–‰
        for model_name, model in kaggle_models.items():
            try:
                logger.info(f"Kaggle ì œì¶œìš© {model_name} ëª¨ë¸ í•™ìŠµ ì¤‘...")
                model.fit(X_train_full, y_train_full)
                logger.info(f"{model_name} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
                
                # ëª¨ë¸ ì €ì¥
                kaggle_model_path = models_dir / f'{model_name}_kaggle_model.pkl'
                with open(kaggle_model_path, 'wb') as f:
                    pickle.dump(model, f)
                logger.info(f"{model_name} Kaggle ì œì¶œìš© ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {kaggle_model_path}")
                
                # Test ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
                predictions = model.predict(X_test)
                logger.info(f"{model_name} ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ìƒ˜í”Œ")
                
                # Submission íŒŒì¼ ìƒì„±
                submission_path = Path(__file__).parent / f'submission_{model_name}.csv'
                submission_df = pd.DataFrame({
                    'PassengerId': self.processed_data.test['PassengerId'].values,
                    'Survived': predictions
                })
                submission_df.to_csv(submission_path, index=False)
                logger.info(f"{model_name} Submission íŒŒì¼ ìƒì„± ì™„ë£Œ: {submission_path}")
                
                results[model_name] = {
                    "model_path": str(kaggle_model_path),
                    "submission_path": str(submission_path),
                    "predictions_count": len(predictions),
                    "survival_count": int(predictions.sum()),
                    "death_count": int((predictions == 0).sum())
                }
            except Exception as e:
                logger.error(f"{model_name} ëª¨ë¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
        
        # ê¸°ë³¸ submission.csvëŠ” ëœë¤í¬ë ˆìŠ¤íŠ¸ ê²°ê³¼ë¡œ ìƒì„±
        if 'RandomForest' in results:
            import shutil
            rf_submission = Path(__file__).parent / 'submission_RandomForest.csv'
            default_submission = Path(__file__).parent / 'submission.csv'
            if rf_submission.exists():
                shutil.copy(rf_submission, default_submission)
                logger.info(f"ê¸°ë³¸ submission.csv ìƒì„± ì™„ë£Œ (RandomForest ê²°ê³¼)")
        
        logger.info("ğŸ˜ğŸ˜ ì œì¶œ ì™„ë£Œ")
        return {
            "models": results,
            "default_submission": "submission.csv (RandomForest ê²°ê³¼)"
        }